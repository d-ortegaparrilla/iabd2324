{"config":{"lang":["es"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Curso IA y Big Data 2023-2024","text":"<p>Materiales para el curso de especializaci\u00f3n de Inteligencia y Artificial.</p>"},{"location":"0101introduccion/","title":"Introducci\u00f3n","text":"<p>Algunos autores comienzan a considerar la inteligencia artificial (Artificial Intelligence, AI) como la nueva revoluci\u00f3n industrial, coraz\u00f3n de lo que algunos llaman industria 4.0.</p> <p>Nos encontramos ante vertiginosos avances en la calidad y prestaciones de una amplia gama de tecnolog\u00edas cotidianas: en el caso del reconocimiento de voz autom\u00e1tica (Automated Speech Recognition, ASR), un asistente virtual de atenci\u00f3n al cliente en el servicio de chat en l\u00ednea de muchas empresas. Estos asistentes virtuales, a menudo impulsados tambi\u00e9n por sistemas de procesado de lenguaje natural, son capaces de comprender y responder a las preguntas y consultas de los clientes de manera natural y conversacional.</p> <p>Tambi\u00e9n ha habido avances espectaculares en el procesado de lenguaje natural (Natural Language Processing, NLP). Por ejemplo, su uso en la detecci\u00f3n y an\u00e1lisis de sentimientos en redes sociales. Muchas empresas y organizaciones utilizan sistemas de NLP para monitorear y comprender las opiniones y emociones expresadas por los usuarios en plataformas como Twitter, Facebook e Instagram.</p> <p>Incluso m\u00e1s relevante en el \u00e1mbito del NLP es lo que ocurre con el texto predictivo y la redacci\u00f3n autom\u00e1tica en proyecto como el GPT de la fundaci\u00f3n OpenAI. Esta avanzada red neuronal basada en el procesamiento de lenguaje natural ha demostrado su val\u00eda en una amplia gama de aplicaciones, desde la generaci\u00f3n de texto automatizada hasta la traducci\u00f3n de idiomas, la creaci\u00f3n de contenido, la atenci\u00f3n al cliente virtual y mucho m\u00e1s. </p> <p>A su vez, los avances en la visi\u00f3n por computador (Computer Vision, CV) tambi\u00e9n son enormes: ahora nuestros ordenadores, por ejemplo, pueden reconocer im\u00e1genes y generar descripciones textuales de su contenido en segundos. O la perfecci\u00f3n que est\u00e1n alcanzando los generadores de rostros artificiales, que permite que se mezclen personajes reales y ficticios con total realismo.</p> <p>Estas tres \u00e1reas (ASR, NLP y CV) son cruciales para dar rienda suelta a las mejoras en rob\u00f3tica, drones o autom\u00f3viles sin conductor. La inteligencia artificial est\u00e1 en el coraz\u00f3n de toda esta innovaci\u00f3n tecnol\u00f3gica.</p>"},{"location":"0102definicion/","title":"Definici\u00f3n de inteligencia artificial.","text":"<p>La Inteligencia Artificial (IA), al igual que la humana, es un concepto complejo de definir. A\u00fan no existe una definici\u00f3n formal y universalmente aceptada.</p> <p>La Comisi\u00f3n Europea la define como sistemas de software (y posiblemente tambi\u00e9n de hardware) dise\u00f1ados por humanos que, ante un objetivo complejo, act\u00faan en la dimensi\u00f3n f\u00edsica o digital:</p> <ul> <li>Percibiendo su entorno, a trav\u00e9s de la adquisici\u00f3n e interpretaci\u00f3n de datos estructurados o no estructurados.</li> <li>Razonando sobre el conocimiento, procesando la informaci\u00f3n derivada de estos datos y decidiendo las mejores acciones para lograr el objetivo dado.</li> </ul> <p>Los sistemas de IA pueden usar reglas simb\u00f3licas o aprender un modelo num\u00e9rico. Tambi\u00e9n pueden adaptar su comportamiento al analizar c\u00f3mo el medio ambiente se ve afectado por sus acciones previas.</p> <p>La inteligencia artificial (IA) es un campo de la inform\u00e1tica que se enfoca en crear sistemas que puedan realizar tareas que normalmente requieren inteligencia humana, como el aprendizaje, el razonamiento y la percepci\u00f3n. Estos sistemas pueden percibir su entorno, razonar sobre el conocimiento, procesar la informaci\u00f3n derivada de los datos y tomar decisiones para lograr un objetivo dado.</p> <p></p>"},{"location":"0103clases/","title":"Clases de inteligencia artificial.","text":"<p>Se escriben muchas exageraciones acerca de la inteligencia artificial en los medios de comunicaci\u00f3n. Una de las razones es que con \"inteligencia artificial\" se expresan muchas cosas, pero podr\u00edamos destacar dos de ellas, dos ideas separadas que se refieren a cosas muy diferentes.</p> <p>Por un lado, casi todo el progreso que estamos viendo en la inteligencia artificial se agrupa en lo que se denomina inteligencia artificial d\u00e9bil (Narrow Artificial Intelligence). Pero con inteligencia artificial tambi\u00e9n nos referimos a un segundo concepto, nombrado inteligencia artificial fuerte (General Artificial Intelligence). Este tipo de inteligencia es aquella que considera que las m\u00e1quinas pueden hacer cualquier cosa que un humano pueda hacer, o ser superinteligentes y hacer incluso m\u00e1s cosas.</p> <p>Si bien hay muchos progresos en el \u00e1rea de la inteligencia artificial d\u00e9bil, no hay casi ninguno en lo que se refiere a la inteligencia artificial fuerte. Pero el r\u00e1pido progreso en la inteligencia artificial d\u00e9bil, que es incre\u00edblemente valioso, ha hecho que los medios de comunicaci\u00f3n a veces concluyan que hay mucho progreso tambi\u00e9n en la fuerte, lo cual no es cierto en estos momentos. La inteligencia artificial fuerte es un \u00e1mbito en el que los investigadores pueden trabajar, pero en el que se est\u00e1 a\u00fan muy lejos de conseguir un gran conocimiento; pueden pasar d\u00e9cadas o cientos de a\u00f1os, qui\u00e9n sabe.</p> <p>Un ejemplo. Incluso un ni\u00f1o de tres a\u00f1os puede aprender cosas de una manera que las computadoras no pueden hacerlo por ahora; una ni\u00f1o de tres a\u00f1os en realidad domina la f\u00edsica intuitivamente. Por ejemplo, sabe perfectamente que cuando tira una bola al aire esta caer\u00e1. O cuando derrama algunos l\u00edquidos espera el desastre resultante. Sus padres no necesitan ense\u00f1arle las leyes de Newton, o hablarle de las ecuaciones diferenciales que definen la trayectoria de los objetos; este ni\u00f1o descubre todas estas cosas solo, sin supervisi\u00f3n.</p> <p>Ahora bien, hay autores que consideran que incluso s\u00f3lo con la inteligencia artificial d\u00e9bil nos dirigimos r\u00e1pidamente hacia una situaci\u00f3n en la que los sistemas inform\u00e1ticos tomar\u00e1n decisiones por nosotros, y piden que nos preguntemos qu\u00e9 suceder\u00e1 cuando esos sistemas dejen de lado la estrategia humana en favor de algo totalmente desconocido para nosotros.</p> <p></p>"},{"location":"0103clases/#el-test-de-turing","title":"El test de Turing.","text":"<p>El matem\u00e1tico brit\u00e1nico Alan Turing (1912-1954) es considerado uno de los padres de las Ciencias de la Computaci\u00f3n. Entre muchas de sus contribuciones formul\u00f3 la prueba que lleva su nombre. Seg\u00fan este investigador, un ordenador que fuese capaz el test de Turing se podr\u00eda considerar inteligente.</p> <p>El test de Turing consiste en hacer que una persona hable a trav\u00e9s de una pantalla y un teclado simult\u00e1neamente con un grupo de individuos entre los que se esconde un ordenador. A la vista de las respuestas que recibe de cada uno de ellos, esta persona deber\u00eda ser capaz de saber qui\u00e9n es el ordenador y qui\u00e9nes son los seres humanos. Si el ordenador no es descubierto, se podr\u00eda considerar que ha pasado el test de Turing. Dado que la conversaci\u00f3n se lleva a cabo en forma de texto, a trav\u00e9s de un teclado y un monitor, no es necesario que la m\u00e1quina sea capaz de transformar el texto en voz, aunque estoy hoy en d\u00eda ya es posible. Esta prueba no eval\u00faa conocimientos, dado que un ser humano no lo sabe todo, sino que lo que mide es la capacidad de una m\u00e1quina de conversar como lo har\u00eda un ser humano.</p> <p>Ya han pasado m\u00e1s de 70 a\u00f1os desde que se enunciara el test de Turing y hay m\u00e1quinas que han conseguido superarla.</p>"},{"location":"0104historia/","title":"Historia de la inteligencia artificial.","text":""},{"location":"0104historia/#inicios-de-la-inteligencia-artificial-optimismo-inicial-desde-los-anos-40-hasta-mediados-de-los-60","title":"Inicios de la inteligencia artificial. Optimismo inicial (desde los a\u00f1os 40 hasta mediados de los 60)","text":"<p>Los fundamentos de los paradigmas de desarrollo de la Inteligencia Artificial (IA) que dominan en la actualidad se originaron en investigaciones que se llevaron a cabo antes de 1956.</p> <p>En 1943, Warren McCulloch y Walter Pitts desarrollaron un modelo pionero basado en la teor\u00eda computacional de Alan Turing. Este modelo consist\u00eda en neuronas artificiales que pod\u00edan activarse o desactivarse seg\u00fan los est\u00edmulos recibidos de neuronas cercanas. Sosten\u00edan la idea de que redes neuronales de este tipo ten\u00edan el potencial de \"aprender\", marcando as\u00ed el surgimiento del paradigma conexionista que se explorar\u00eda posteriormente.</p> <p>En ese mismo a\u00f1o, se introdujeron los principios del paradigma simb\u00f3lico al interpretar el conocimiento humano en t\u00e9rminos de descripciones declarativas y m\u00f3dulos de entidades simb\u00f3licas de alto nivel, junto con un conjunto de reglas de inferencia para manipular estas descripciones simb\u00f3licas.</p> <p>En 1950, el trabajo de Alan Turing complement\u00f3 a\u00fan m\u00e1s los cimientos del paradigma simb\u00f3lico al proponer un m\u00e9todo experimental para evaluar la inteligencia contenida en un \"programa de IA\", que ahora conocemos como el \"Test de Turing\". Tambi\u00e9n introdujo conceptos como el aprendizaje autom\u00e1tico, los algoritmos gen\u00e9ticos y el aprendizaje por refuerzo en su obra.</p> <p>En 1956, se lleg\u00f3 a un consenso sobre la adopci\u00f3n del t\u00e9rmino \"Inteligencia Artificial\" para englobar los avances en estos campos. Esto condujo al desarrollo de herramientas como el Sistema de Resoluci\u00f3n General de Problemas (SRGP), dise\u00f1ado para imitar los procesos de resoluci\u00f3n de problemas humanos, y el lenguaje de programaci\u00f3n LISP, que domin\u00f3 la programaci\u00f3n en IA en esos primeros a\u00f1os.</p> <p>El \u00e9xito de estos avances durante este per\u00edodo gener\u00f3 predicciones optimistas y a corto plazo sobre el futuro desarrollo de la IA. Sin embargo, como veremos m\u00e1s adelante, estas predicciones chocaron con la realidad al enfrentarse a problemas m\u00e1s complejos y variados en las etapas posteriores.</p>"},{"location":"0104historia/#la-epoca-oscura-mediados-de-los-60-hasta-mediados-de-los-70","title":"La \u00e9poca oscura (mediados de los 60 hasta mediados de los 70)","text":"<p>Durante este per\u00edodo, se lleg\u00f3 a reconocer que el mundo real era considerablemente m\u00e1s complejo de lo que se hab\u00eda anticipado en etapas anteriores, lo que llev\u00f3 al abandono o desaceleraci\u00f3n de numerosas l\u00edneas de investigaci\u00f3n previas.</p> <p>Los esfuerzos cient\u00edficos y financieros se enfocaron de manera m\u00e1s cautelosa en el dise\u00f1o de t\u00e9cnicas para representar el conocimiento, con el prop\u00f3sito de utilizarlas posteriormente en procesos de inferencia. Paralelamente, se desarrollaron lenguajes de programaci\u00f3n espec\u00edficos para cada una de estas t\u00e9cnicas de representaci\u00f3n, como Prolog para la l\u00f3gica y Lisp para las reglas.</p> <p>En este per\u00edodo se acept\u00f3 que muchos de los problemas que se hab\u00edan intentado abordar mediante la Inteligencia Artificial resultaban, en la pr\u00e1ctica, intratables. Es decir, no exist\u00eda una base cient\u00edfica s\u00f3lida para resolver estos problemas mediante enfoques de ingenier\u00eda. Un ejemplo de esto fue el intento de desarrollar sistemas de traducci\u00f3n autom\u00e1tica, una idea atractiva en una \u00e9poca marcada por la Guerra Fr\u00eda y la inversi\u00f3n en proyectos de \u00edndole militar.</p> <p>Inicialmente, se cre\u00eda err\u00f3neamente que la limitaci\u00f3n principal resid\u00eda en la capacidad de c\u00e1lculo de las m\u00e1quinas de la \u00e9poca. Sin embargo, con el tiempo se comprob\u00f3 que, a pesar de dedicar un gran n\u00famero de horas de procesamiento a la resoluci\u00f3n de problemas, no se lograban alcanzar los objetivos planteados.</p>"},{"location":"0104historia/#renacimiento-de-la-ia-finales-de-los-anos-70","title":"Renacimiento de la IA (finales de los a\u00f1os 70)","text":"<p>En esta etapa, se volvieron a enfocar los esfuerzos de desarrollo en la soluci\u00f3n de problemas utilizando sistemas expertos que se basaban en reglas y principios cient\u00edficos claramente definidos. Como resultado, comenzaron a surgir programas especializados para dominios muy espec\u00edficos. Por ejemplo, programas como DENDRAL y Mycin pudieron deducir la estructura de ciertas mol\u00e9culas a partir de datos de espectrometr\u00eda de masas y diagnosticar enfermedades infecciosas en muestras de sangre a partir de datos m\u00e9dicos, respectivamente.</p> <p>Este aumento generalizado en la aplicaci\u00f3n de la IA para abordar problemas del mundo real condujo a una mayor demanda de esquemas de representaci\u00f3n del conocimiento efectivos, lo que a su vez dio lugar al desarrollo de nuevos lenguajes de programaci\u00f3n.</p>"},{"location":"0104historia/#desarrollo-industrial-y-economico-de-los-80-hasta-la-actualidad","title":"Desarrollo industrial y econ\u00f3mico (de los 80 hasta la actualidad)","text":"<p>Comienzan a desarrollarse sistemas expertos aplicados a la industria que suponen, en la pr\u00e1ctica, un considerable beneficio econ\u00f3mico para las empresas que los implementan.</p> <p>A mediados de los a\u00f1os 80 se recuper\u00f3 el planteamiento de redes neuronales que dio base al modelo conexionista al redise\u00f1ar el algoritmo de aprendizaje con realimentaci\u00f3n aplic\u00e1ndolo a diversos problemas de aprendizaje en los campos de la inform\u00e1tica y la psicolog\u00eda. </p> <p>En los \u00faltimos a\u00f1os y hasta nuestros d\u00edas se han centrado los esfuerzos en el \u00e1rea de la IA en la mejora de las teor\u00edas ya existentes, en lugar de tratar de idear nuevas v\u00edas de desarrollo sin una base experimental y cient\u00edfica s\u00f3lidas. </p> <p>La IA ya ha adoptado el m\u00e9todo cient\u00edfico, por lo que las hip\u00f3tesis que postula se deben someter a rigurosos experimentos emp\u00edricos, y los resultados deben analizarse estad\u00edsticamente para identificar su relevancia. </p> <p>Se est\u00e1n desarrollando sistemas de razonamiento incierto como las redes de Bayes que permiten facilitar la representaci\u00f3n eficiente y el razonamiento riguroso en situaciones y problemas de incertidumbre. </p> <p>Los principales avances ocurridos en las \u00faltimas d\u00e9cadas ser\u00edan:</p> <ul> <li>D\u00e9cada de 2000: Avances en el aprendizaje profundo permiten mejoras en reconocimiento de voz y visi\u00f3n artificial.</li> <li>D\u00e9cada de 2010: La IA se convierte en parte integral de la vida cotidiana a trav\u00e9s de asistentes virtuales y veh\u00edculos aut\u00f3nomos.</li> <li>D\u00e9cada de 2020: La IA se utiliza en la medicina, la investigaci\u00f3n cient\u00edfica y la toma de decisiones empresariales.</li> </ul> <p>El futuro de la IA promete avances emocionantes en campos como la rob\u00f3tica, la IA cu\u00e1ntica y la \u00e9tica de la inteligencia artificial. A medida que la tecnolog\u00eda contin\u00faa evolucionando, la inteligencia artificial seguir\u00e1 transformando la forma en que vivimos y trabajamos.</p>"},{"location":"0105campos/","title":"Campos de aplicaci\u00f3n de la inteligencia de artificial.","text":"<p>A continuaci\u00f3n, vamos a entrar en detalle en los distintos campos de aplicaci\u00f3n de la inteligencia artificial.</p>"},{"location":"0105campos/#vision-artificial","title":"Visi\u00f3n artificial","text":"<p>La visi\u00f3n artificial ha experimentado una transformaci\u00f3n radical gracias al aprendizaje profundo y las redes convolucionales en la \u00faltima d\u00e9cada. Estos avances han dado lugar a una amplia gama de aplicaciones, que van desde la clasificaci\u00f3n de im\u00e1genes hasta la identificaci\u00f3n de rostros, pasando por la detecci\u00f3n de objetos y el seguimiento de movimientos.</p> <p>En el \u00e1mbito de la clasificaci\u00f3n de im\u00e1genes, se han desarrollado algoritmos que pueden distinguir y categorizar objetos en fotograf\u00edas, como reconocer la diferencia entre perros y gatos o interpretar los n\u00fameros en las matr\u00edculas de los veh\u00edculos.</p> <p>Los algoritmos de seguimiento han permitido que drones equipados con c\u00e1maras realicen b\u00fasquedas y rastreos de personas desaparecidas, y sigan a posibles delincuentes. Combinados con c\u00e1maras t\u00e9rmicas, estos algoritmos se han convertido en herramientas de seguridad esenciales, utilizadas para supervisar \u00e1reas naturales y bosques con el objetivo de prevenir incendios.</p> <p>En el \u00e1mbito de la seguridad, los detectores de objetos se utilizan en sistemas de inspecci\u00f3n para evitar la introducci\u00f3n de armas u objetos prohibidos en trenes y aviones.</p> <p>Los segmentadores de im\u00e1genes permiten aislar objetos del fondo en una imagen, y esta tecnolog\u00eda se emplea con \u00e9xito en la construcci\u00f3n de modelos tridimensionales de \u00f3rganos a partir de im\u00e1genes m\u00e9dicas bidimensionales.</p> <p>Los identificadores de rostros, una forma especializada de los detectores de caras, posibilitan la creaci\u00f3n de sistemas avanzados de control de acceso. Un ejemplo destacado es un ascensor desarrollado por la empresa asturiana \"ATI Ascensores\" que puede identificar autom\u00e1ticamente a los residentes y llevarlos a su destino, lo que representa un avance importante para las personas con discapacidades motoras o sensoriales.</p> <p>Las redes generativas antag\u00f3nicas (Generative Adversarial Networks, GAN) han causado un impacto revolucionario en la industria cinematogr\u00e1fica, permitiendo la creaci\u00f3n de rostros extremadamente realistas de personas reales, incluso despu\u00e9s de su fallecimiento. Sin embargo, tambi\u00e9n han dado lugar a la proliferaci\u00f3n del software DeepFake, que puede modificar caras en im\u00e1genes y videos para hacer parecer que una persona es otra, inundando las tiendas de aplicaciones de dispositivos m\u00f3viles con estas herramientas.</p> <p>Adem\u00e1s, los autoencoders se utilizan para mejorar la calidad de las fotograf\u00edas y videos, eliminando el ruido y restaurando la claridad de las im\u00e1genes.</p>"},{"location":"0105campos/#proceso-y-adquision-de-video","title":"Proceso y adquisi\u00f3n de v\u00eddeo","text":"<p>Al igual que en el anterior apartado, el procesamiento de videos (secuencias de im\u00e1genes en movimiento) y la representaci\u00f3n de escenas mediante t\u00e9cnicas como el trazado de rayos, el superescalado, DLSS y otros algoritmos de mejora han marcado un nuevo est\u00e1ndar en la industria de los videojuegos. Esto ha llevado la calidad de la gran pantalla directamente al hogar, aunque es importante tener en cuenta que los requisitos de hardware para lograrlo no deben subestimarse. Por ejemplo, para llevar a cabo estas tareas en videojuegos con resoluci\u00f3n Full HD o 2K (dependiendo del t\u00edtulo), se requiere una tarjeta gr\u00e1fica potente y moderna, como una RTX3060 de la familia Ampere o equivalente.</p> <p>El trazado de rayos (Ray Tracing) utiliza algoritmos avanzados que permiten determinar los lugares donde la luz se refleja o refracta para representar de manera precisa los reflejos y refracciones en tiempo real. En la d\u00e9cada de 2000, esta t\u00e9cnica se usaba principalmente para generar gr\u00e1ficos est\u00e1ticos como apoyo a la venta, por ejemplo, para renderizar im\u00e1genes de proyectos urban\u00edsticos complejos. Esto se hac\u00eda utilizando m\u00faltiples tarjetas gr\u00e1ficas en paralelo a trav\u00e9s de tecnolog\u00edas como SLI (Scalable Link Interface) o CrossFire.</p> <p>El superescalado (Supersampling) en t\u00e9rminos sencillos implica mejorar artificialmente una imagen para obtener una mayor resoluci\u00f3n en una pantalla, como una pantalla 4K, sin sacrificar la velocidad de cuadros por segundo (fps) generada por la tarjeta gr\u00e1fica. Existen diversas tecnolog\u00edas para lograrlo, siendo la m\u00e1s establecida la de NVIDIA con su Deep Learning Super Sampling (DLSS), seguida de cerca por AMD con FidelityFX Super Resolution (FSR).</p> <p>Adem\u00e1s, cabe mencionar DeepStream, que aunque no est\u00e1 directamente relacionado con las tecnolog\u00edas mencionadas anteriormente, consiste en un conjunto de bibliotecas de procesamiento de video que se est\u00e1 implementando en aplicaciones industriales para el control, an\u00e1lisis y procesamiento de videos, as\u00ed como datos de sensores \u00f3pticos.</p>"},{"location":"0105campos/#reconocimiento-de-voz-y-lenguaje-natural","title":"Reconocimiento de voz y lenguaje natural.","text":"<p>Estas dos disciplinas est\u00e1n estrechamente relacionadas entre s\u00ed, al punto que varios expertos consideran el reconocimiento autom\u00e1tico del habla (ASR, Automatic Speech Recognition) como una subdisciplina del procesamiento de lenguaje natural (NLP, Natural Language Processing).</p> <p>El reconocimiento de voz y el procesamiento de lenguaje natural permiten buscar informaci\u00f3n espec\u00edfica, realizar traducciones autom\u00e1ticas y ofrecer una interfaz amigable para asistentes virtuales y chatbots.</p> <p>Los productos rob\u00f3ticos basados en la plataforma NVIDIA Jetson pueden aprovechar algoritmos de inteligencia artificial tanto en unidades de procesamiento gr\u00e1fico (GPU) como en unidades de procesamiento central (CPU) para llevar a cabo tareas de reconocimiento de voz y NLP. Es importante se\u00f1alar que el uso de NLP no siempre implica la utilizaci\u00f3n de inteligencia artificial, como se puede ver al emplear la Software Development Kit (SDK) de NLTK (Natural Language Tool Kit). En el caso de las tecnolog\u00edas propietarias de NVIDIA, NeMo y Jarvis son las interfaces de programaci\u00f3n de aplicaciones (API) encargadas de estas tareas, e incluso incluyen un motor de s\u00edntesis de texto a voz (TTS).</p>"},{"location":"0105campos/#asistentes-virtuales-y-recomendadores","title":"Asistentes virtuales y recomendadores.","text":"<p>En diversas modalidades, asistentes virtuales como Alexa, Siri o Google est\u00e1n ingresando en los hogares con el prop\u00f3sito de simplificar la automatizaci\u00f3n del entorno dom\u00e9stico a un costo asequible.</p> <p>Los sistemas de recomendaci\u00f3n y otros chatbots tienen la funci\u00f3n de desempe\u00f1ar el rol de asistentes de ventas. Cuando se combinan con t\u00e9cnicas de visi\u00f3n mejorada y realidad aumentada, permiten a los usuarios visualizar c\u00f3mo les quedar\u00eda una prenda espec\u00edfica o les sugieren otros productos que podr\u00edan interesarles, bas\u00e1ndose en la relaci\u00f3n de datos de compras realizadas por otros usuarios que han adquirido productos similares.</p>"},{"location":"0105campos/#ciencias-de-datos-y-data-mining","title":"Ciencias de datos y Data Mining","text":"<p>Dentro del \u00e1mbito de la ciencia de datos, la inteligencia artificial ha tenido un impacto significativo al permitir la identificaci\u00f3n de patrones y relaciones a trav\u00e9s de m\u00e9todos no supervisados. Adem\u00e1s, facilita la realizaci\u00f3n de agrupaciones y aplicaciones heur\u00edsticas. Existen diversos algoritmos y herramientas disponibles, como por ejemplo, nVidia RAPIDS, que utiliza los n\u00facleos CUDA de una unidad de procesamiento gr\u00e1fico (GPU) para acelerar el proceso de entrenamiento y la inferencia en redes neuronales. En este campo, tambi\u00e9n se incluyen heur\u00edsticas y sistemas de detecci\u00f3n de anomal\u00edas que se aplican en el \u00e1mbito del mantenimiento industrial.</p>"},{"location":"0105campos/#ciberseguridad","title":"Ciberseguridad.","text":"<p>Sistemas como nVidia Morpheus pueden hacer las funciones de un IDS (Intruder Detector System) mediante la b\u00fasqueda de anomal\u00edas en el tr\u00e1fico de red.</p>"},{"location":"0201introduccion/","title":"Introducci\u00f3n","text":"<p>En la era digital actual, los modelos de inteligencia artificial (IA) se han convertido en protagonistas indiscutibles, desempe\u00f1ando un papel fundamental en una amplia variedad de aplicaciones y sectores, desde la asistencia virtual en el hogar hasta diagn\u00f3sticos m\u00e9dicos avanzados y la optimizaci\u00f3n de la cadena de suministro. Estos modelos son el resultado de d\u00e9cadas de investigaci\u00f3n en aprendizaje autom\u00e1tico, procesamiento de lenguaje natural, visi\u00f3n por computadora y m\u00e1s.</p> <p>En este apartado se har\u00e1 un breve repaso por algunos de los modelos existentes en Inteligencia Artificial, detallando no s\u00f3lo aquellos pertenecientes al \u00e1mbito del Machine Learning.</p>"},{"location":"0202problemas/","title":"Sistemas de resoluci\u00f3n de problemas","text":"<p>Todo sistema de resoluci\u00f3n de problemas debe ser capaz de evolucionar hasta alcanzar una de las posibles soluciones del problema.</p> <p>Los sistemas de resoluci\u00f3n de problemas tienen como objetivo alcanzar alguno de los estados en los que el problema se puede considerar resuelto. A la hora de formular que el sistema sea capaz de alcanzar la soluci\u00f3n, es necesario que el medio donde \u00e9ste se formula cumpla una serie de caracter\u00edsticas:</p> <ul> <li>En primer lugar, que sea observable para el sistema de resoluci\u00f3n de problemas. Es decir, que de alguna forma este sistema pueda reconocer y explorar el medio en su totalidad.</li> <li>En segundo lugar, resulta conveniente que el medio sea finito y determinista, entendiendo como tal que ejercer la misma acci\u00f3n sobre el medio, conduzca siempre al mismo resultado.</li> </ul> <p>As\u00ed, si el medio es observable, finito y determinista, la resoluci\u00f3n de cualquier problema consistir\u00e1 en la ejecuci\u00f3n de un n\u00famero finito de pasos.</p> <p>El proceso mediante el que, haciendo uso de una secuencia de acciones, el sistema de resoluci\u00f3n de problemas alcanza el objetivo, se denomina b\u00fasqueda.</p> <p>Este tipo de sistemas disponen de los siguientes componentes:</p> <ul> <li>Estado inicial: se trata del estado desde el que se comenzar\u00e1 el proceso de b\u00fasqueda.</li> <li>Acciones que se pueden ejecutar: en funci\u00f3n del problema con el que se est\u00e9 trabajando, el sistema dispondr\u00e1 de una serie de posibles acciones que le permitir\u00e1n pasar de un estado a otro.</li> <li>Modelo de transici\u00f3n: explica lo que har\u00e1 cada una de esas acciones.</li> <li>Espacio de estados del problema: se trata del conjunto de todas las posibles situaciones a las que se puede llegar a partir del punto de partida.</li> <li>Verificaci\u00f3n de que se ha alcanzado el objetivo, que determina si la posici\u00f3n lograda es una de las posiciones objetivo.</li> <li>Coste de ejecuci\u00f3n: en muchos casos, puede resultar de inter\u00e9s el saber cu\u00e1nto cuesta ejecutar cada una de las acciones que son necesarias para llegar al objetivo.</li> </ul>"},{"location":"0202problemas/#ejemplo-juego-de-varios-competidores-tres-en-raya","title":"Ejemplo. Juego de varios competidores: tres en raya","text":"<p>Vamos a ver un problema cl\u00e1sico de la IA: los juegos. La situaci\u00f3n m\u00e1s sencilla, en la que nos centraremos en aras de la claridad, son los juegos de dos jugadores de informaci\u00f3n perfecta, como el tres en raya y el ajedrez.</p>"},{"location":"0202problemas/#arboles-de-juego","title":"\u00c1rboles de juego","text":"<p>Los distintos estados del juego se representan mediante nodos en el \u00e1rbol de juego. En el \u00e1rbol de juego, los nodos est\u00e1n dispuestos en niveles que se corresponden con los turnos de cada jugador, de forma que el nodo \u00abra\u00edz\u00bb del \u00e1rbol (normalmente representado en la parte superior del diagrama) es la posici\u00f3n inicial en el juego. En el tres en raya, ser\u00eda la cuadr\u00edcula vac\u00eda, sin X ni O. Debajo de la ra\u00edz, en el segundo nivel, se representan los estados que pueden derivarse de los primeros movimientos de los jugadores, ya sea X o O. Nos referimos a estos nodos como los \u00abhijos\u00bb del nodo ra\u00edz.</p> <p>A su vez, cada nodo del segundo nivel tendr\u00e1 como nodos hijos los estados que pueden resultar de \u00e9l en funci\u00f3n de los movimientos del jugador contrincante. Esta situaci\u00f3n prosigue, nivel a nivel, hasta alcanzar estados en los que el juego finaliza. En el tres en raya, esto significa que uno de los jugadores consigue colocar tres fichas en l\u00ednea y gana, o que el tablero est\u00e1 completo y el juego termina en empate.</p> <p></p>"},{"location":"0202problemas/#valor-minimizante-y-maximizante","title":"Valor m\u00ednimizante y maximizante","text":"<p>Para desarrollar un m\u00e9todo de IA que trate de ganar el juego, asignamos un valor num\u00e9rico a cada resultado final posible. A las posiciones del tablero en las que hay tres X en raya de forma que Max gana les asignamos el valor +1, y, del mismo modo, a las posiciones en que Min gana con tres O en raya les asignamos el valor -1. Para las posiciones en que el tablero est\u00e1 lleno y ninguna de las jugadoras gana, utilizamos el valor neutral 0 (realmente no importa cu\u00e1les sean los valores mientras sigan este orden, de forma que Max intente maximizar el valor y Min, minimizarlo).</p> <p> </p>"},{"location":"0202problemas/#crecimiento-del-arbol-ramificacion-y-poda","title":"Crecimiento del \u00e1rbol. Ramificaci\u00f3n y poda","text":"<p>Un problema com\u00fan es el r\u00e1pido crecimiento del \u00e1rbol de estados. \u00a1En el problema del tres en raya, el espacio de estados es 9! (factorial de 9), o lo que es lo mismo 9x8x7x6x5x4x3x2x1=362.880 jugadas posibles.</p> <p>En el caso del ajedrez, se considera que cada jugada tiene un nivel de multiplicaci\u00f3n de 35, es decir, 35 alternativas por cada estado. En dos niveles, ser\u00eda 35x35=1.225 movimientos posibles. En 5 niveles tendr\u00edamos 52.521.875 movimientos. En 10 niveles m\u00e1s de 2.700 millones de jugadas.</p> <p>Por muy r\u00e1pido que sea el sistema inform\u00e1tico, la posibilidad de explorar todo el \u00e1rbol de decisi\u00f3n es pr\u00e1cticamente imposible.</p> <p>Hace falta alg\u00fan tipo de algoritmo que limite la b\u00fasqueda en el espacio de estados. Algunos de estos algoritmos ser\u00edan el de poda alfa-beta o el de poda heur\u00edstica.</p>"},{"location":"0203expertos/","title":"Sistemas basados en reglas. Sistemas expertos","text":"<p>El dise\u00f1o de los sistemas basados en reglas, m\u00e1s conocidos como sistemas expertos, hacen uso de un conjunto de reglas del tipo SI\u2026ENTONCES, conocidas en ingl\u00e9s como IF\u2026THEN. Mediante este conjunto de reglas pueden realizar deducciones o elegir entre distintas alternativas. Estos sistemas expertos son programas de ordenador capaces de simular algunas de las caracter\u00edsticas del conocimiento humano con el fin de realizar tareas que normalmente llevan a cabo \u00fanicamente personas expertas.</p> <p>Todo sistema basado en reglas consta, al menos, de cuatro componentes principales:</p> <ul> <li>Una lista de reglas, que constituye la base de datos de conocimiento que emplear\u00e1.</li> <li>Un motor de inferencias que combina la informaci\u00f3n disponible en cada momento con la lista de reglas para producir el razonamiento.</li> <li>Un sistema para la explicaci\u00f3n de las decisiones tomadas debe disponer de alg\u00fan tipo de subsistema que permita presentar una explicaci\u00f3n de las decisiones tomadas de manera que resulte comprensible para el usuario.</li> <li>Un sistema para la adquisici\u00f3n de nuevo conocimiento que permita a un experto en el campo introducir nueva informaci\u00f3n en el sistema.</li> <li>Un interfaz de usuario que permita alg\u00fan tipo de conexi\u00f3n exterior.</li> </ul> <p>La ventaja fundamental de este tipo de sistemas es que tienen un mecanismo cuyo funcionamiento es f\u00e1cil de entender, se pueden construir haciendo uso de conocimiento experto y se aplican en cualquier campo.</p> <p>Entre las desventajas hay que destacar que antes problemas dif\u00edciles, la generaci\u00f3n de reglas se puede volver un proceso de alta complejidad.</p> <p>Los sistemas expertos desarrollados en los \u00faltimos a\u00f1os no se limitan a la implementaci\u00f3n de reglas, sino que las combinan con metodolog\u00edas propias del Machine Learning.</p>"},{"location":"0204impreciso/","title":"Sistemas de razonamiento impreciso","text":"<p>Los sistemas de razonamiento pueden describirse por la exactitud que necesitan al realizar cualquier paso de su proceso de razonamiento. Los sistemas de razonamiento m\u00e1s preciso son aquellos que s\u00f3lo se ocupan de las relaciones l\u00f3gicamente v\u00e1lidas, conocidas con certeza. En estos casos, las conclusiones son verdaderas, si las premisas son verdaderas.</p> <p>Existen metodolog\u00edas que permiten el razonamiento bajo condiciones de incertidumbre. El uso de estos sistemas cobra gran importancia cuando se trata de construir agentes que van a operar en situaciones reales en las que, por tanto, se manejar\u00e1 incertidumbre.</p> <p>Entre las aproximaciones existentes para el manejo de la incertidumbre se incluyen el uso de m\u00e9todos probabil\u00edsticos como las redes bayesianas y la l\u00f3gica difusa.</p>"},{"location":"0204impreciso/#redes-bayesianas","title":"Redes bayesianas","text":"<p>El teorema de Bayes nos permite actualizar las probabilidades de variables cuyo estado no hemos observado dada una serie de nuevas observaciones. Las redes bayesianas automatizan este proceso, permitiendo que el razonamiento avance en cualquier direcci\u00f3n a trav\u00e9s de la red de variables. Las redes bayesianas est\u00e1n constituidas por una estructura en forma de grafo, en la que cada nodo representa variables aleatorias (discretas o continuas) y cada arista representa las conexiones directas entre ellas.</p> <p>No entraremos en detalle del funcionamiento de una red bayesiana, aunque s\u00ed veremos el teorema de Bayes, que se usa en numerosos \u00e1mbitos de la vida real.</p> <p>La probabilidad de ocurrencia de un evento A se expresa como:</p> <p>$$ P(A) = {n\u00ba\\_ocurrencias\\_A \\over n\u00ba\\_total\\_eventos} $$</p> <p>La probabilidad de ocurrencia de un evento A condicionada a que ocurra otro evento B se especifica mediante el teorema de Bayes de la forma:</p> <p>$$ P(A|B) = {P(B|A) * P(A) \\over P(B)} $$</p>"},{"location":"0204impreciso/#ejemplo-infeccion-zombi","title":"Ejemplo. Infecci\u00f3n zombi","text":"<p>Una nueva epidemia se extiende por Elche. Los afectados se convierten en zombis apenas un par de d\u00edas despu\u00e9s del contagio. La prevalencia de esta epidemia es de 5 infectados por cada mil habitantes. Para intentar controlar la epidemia se ha desarrollado un test de ant\u00edgenos muy preciso, con una sensibilidad del 99.9% y especificidad de 99.0%. Realmente es un test muy bueno. Cuando hay infecci\u00f3n zombi, el test da positivo el 99.9% de las veces y negativo el 0.1%. Cuanto no hay infecci\u00f3n zombi, el test da negativo el 99.0% de las veces y positivo el 1.0%.</p> <p>Lo podemos ver mejor en la siguiente tabla:</p> Infecci\u00f3n zombi No infecci\u00f3n Test positivo 0,999 (verdaderos positivos) 0,010 (falsos positivos) Test negativo 0,001 (falsos negativos) 0,990 (verdaderos negativos) <p>Has ido a una fiesta y sospechas que puedes haberte contagiado, pero no sabes si est\u00e1s infectado o no, por lo que recurres a hacerte un test. Lamentablemente, el test da un resultado POSITIVO.</p> <p>PANICO: el test te est\u00e1 diciendo que con un 99.9% de probabilidad est\u00e1s contagiado, eso es casi una certeza. Una vez superado el shock inicial, asumes lo inevitable y te preparas para lo peor, pero hay algo que no cuadra\u2026</p> <p>Si hubiese recurrido a Bayes desde el principio me habr\u00eda ahorrado el susto:</p> <p>$$ P(infeccion|test\\_positivo) = {P(test\\_positivo|infeccion) * P(infeccion) \\over P(test\\_positivo)} $$</p> <p>$$ P(test\\_positivo) = P(verdadero\\_positivo) * P(infeccion) + P(falso\\_positivo ) \u2217 P(no\\_infeccion)$$</p> <p>$$ P(test\\_positivo) = 0.999 \u2217 0.005 + 0.01 \u2217 0.995 = 0.015 $$</p> <p>$$ P(infeccion|test\\_positivo)= {0.999 \u2217 0.005 \\over 0.015} = 0.33 $$</p> <p>\u00a1Menudo susto! Tengo un 33% de probabilidad de estar infectado. Mis posibilidades son mucho mejores de lo hab\u00eda imaginado inicialmente.</p>"},{"location":"0204impreciso/#logica-difusa","title":"L\u00f3gica difusa","text":"<p>La l\u00f3gica difusa es capaz de procesar distintos grados de verdad, pero estos no deben ser interpretados en el sentido de probabilidades, pues lo que representan dentro del \u00e1mbito de la l\u00f3gica difusa es la probabilidad de pertenencia a cierto conjunto, en vez de la probabilidad de que ocurra un evento.</p> <p>Un ejemplo. Si se dispone de una botella numerada como 1 con agua que tiene un grado de pertenencia difuso del 80% al conjunto de agua potable. Por otro lado, la botella numerada como 2 tiene una probabilidad del 80% de ser potable. \u00bfDe qu\u00e9 botella es menos arriesgado beber?</p> <p>Se ha de interpretar que la primera botella tiene un contenido de agua que es bastante similar al de otras botellas que son potables, alrededor de un 80%, mientras que en el caso de la segunda botella lo que se quiere decir es que, si se bebe el agua de una botella de ese tipo, el 80% de las veces que se han analizado conten\u00edan agua potable. Pero cuidado, que en un 20% de las ocasiones su contenido completo era de agua no potable.</p>"},{"location":"0205machine/","title":"Sistemas de aprendizaje autom\u00e1tico. Machine Learning","text":"<p>El Machine Learning, traducido al castellano como aprendizaje autom\u00e1tico, es la ciencia de programar ordenadores para que aprendan a partir de datos. Esto otorga a los ordenadores la capacidad de aprender sin ser programados de manera expl\u00edcita.</p> <p>Se dice que un programa de ordenador aprende de la experiencia E, con respecto a una tarea T y una medida de rendimiento R, si su rendimiento en T, medido por P, mejora con la experiencia E.</p> <p>El filtro de spam es un programa de machine learning que, al recibir ejemplos de correo basura (marcados por los usuarios) y ejemplos de correos corrientes (que no sean spam, tambi\u00e9n llamados \u201cham\u201d), puede aprender a marcar el spam. Los ejemplos que el sistema utiliza para aprender se llaman conjunto de entrenamiento. Cada ejemplo de entrenamiento se llama instancia de entrenamiento (o muestra). La parte de un sistema de machine learning que aprende y realiza predicciones se denomina modelo. Las redes neuronales y los random forests son ejemplos de modelos.</p> <p>En este caso, la tarea T es marcar el spam para los correos nuevos, la experiencia E son los datos de entrenamiento y la medida del rendimiento tiene que definirse; por ejemplo, podemos utilizar la proporci\u00f3n de correos clasificados correctamente.</p>"},{"location":"0205machine/#deep-learning","title":"Deep Learning","text":"<p>Como hemos mencionado anteriormente las redes neuronales son un modelo dentro de los sistemas de Machine Learning. Estas redes neuronales artificiales pretenden imitar en cierta manera la actividad de las capas de neuronas en la neocorteza del cerebro humano donde ocurre el pensamiento. Estas redes neuronales artificiales se organizan jer\u00e1rquicamente en capas de procesamiento (construidas con neuronas artificiales). Una red neuronal se considera Deep Learning cuando tiene una o m\u00e1s capas ocultas.</p> <p></p> <p>Los grandes avances en reconocimiento de voz, procesado de lenguaje natural o visi\u00f3n por computador, son debidos en gran parte a los avances del Deep Learning en esta \u00faltima d\u00e9cada.</p> <p></p>"},{"location":"0301introduccion/","title":"Introducci\u00f3n","text":"<p>Podemos clasificar los sistemas de machine learning seg\u00fan los siguientes criterios:</p> <ul> <li>C\u00f3mo se supervisan durante el entrenamiento (aprendizaje supervisado, aprendizaje no supervisado, aprendizaje semisupervisado, aprendizaje autosupervisado y otros).</li> <li>Si pueden o no aprender de forma gradual sobre la marcha (aprendizaje online frente a aprendizaje por lotes).</li> <li>Si funcionan comparando simplemente puntos de datos nuevos con puntos de datos conocidos o si detectan patrones en los datos de entrenamiento y crean un modelo predictivo, como hacen los cient\u00edficos (aprendizaje basado en instancias frente a aprendizaje basado en modelos).</li> </ul>"},{"location":"0302supervision/","title":"Supervisi\u00f3n del entrenamiento","text":"<p>Los sistemas de machine learning pueden clasificarse seg\u00fan la cantidad y el tipo de supervisi\u00f3n que tengan durante el entrenamiento.</p>"},{"location":"0302supervision/#aprendizaje-supervisado","title":"Aprendizaje supervisado","text":"<p>En el aprendizaje supervisado, el conjunto de entrenamiento que introducimos en el algoritmo incluye las soluciones deseadas, denominadas \u201cetiquetas\u201d.</p> <p></p> <p>Una tarea com\u00fan en el aprendizaje supervisado es la clasificaci\u00f3n. Un ejemplo ilustrativo es el filtro de correo no deseado (spam): se entrena utilizando una gran cantidad de correos electr\u00f3nicos de muestra junto con su categorizaci\u00f3n (spam o leg\u00edtimo), y debe aprender a clasificar nuevos correos electr\u00f3nicos.</p> <p>Otra tarea frecuente implica predecir un valor num\u00e9rico objetivo, como el precio de un autom\u00f3vil, cuando se proporciona un conjunto de caracter\u00edsticas (kilometraje, antig\u00fcedad, marca, etc.). Este tipo de tarea se conoce como regresi\u00f3n. Para entrenar el sistema en esta tarea, se le proporcionan numerosos ejemplos de autom\u00f3viles, junto con sus caracter\u00edsticas y sus respectivos objetivos (es decir, sus precios).</p> <p></p> <p>Es importante destacar que algunos modelos de regresi\u00f3n pueden utilizarse igualmente para la clasificaci\u00f3n, y viceversa. Por ejemplo, la regresi\u00f3n log\u00edstica suele emplearse en tareas de clasificaci\u00f3n, ya que puede generar una salida que corresponde a la probabilidad de pertenecer a una clase espec\u00edfica (por ejemplo, un 20% de probabilidad de ser spam).</p>"},{"location":"0302supervision/#aprendizaje-no-supervisado","title":"Aprendizaje no supervisado","text":"<p>En el aprendizaje no supervisado, como podr\u00e1s suponer, los datos de entrenamiento no est\u00e1n etiquetados. El sistema intenta aprender sin profesor.</p> <p></p> <p>Supongamos que disponemos de una gran cantidad de datos sobre los visitantes de nuestro blog. En este escenario, podemos aplicar un algoritmo de agrupamiento con el fin de detectar grupos de visitantes que tengan caracter\u00edsticas similares. No es necesario proporcionar al algoritmo informaci\u00f3n sobre a qu\u00e9 grupo pertenece cada visitante, ya que el algoritmo descubrir\u00e1 estas conexiones por s\u00ed mismo. Por ejemplo, podr\u00eda identificar que el 40% de los visitantes son adolescentes amantes de los c\u00f3mics que suelen leer el blog despu\u00e9s de la escuela, mientras que el 20% son adultos aficionados a la ciencia ficci\u00f3n y visitan el blog durante los fines de semana. Utilizando un algoritmo de agrupamiento jer\u00e1rquico, tambi\u00e9n podr\u00eda subdividir estos grupos en subgrupos m\u00e1s peque\u00f1os. Esta informaci\u00f3n resulta \u00fatil para adaptar nuestras publicaciones a cada grupo de visitantes.</p> <p></p> <p>Los algoritmos de visualizaci\u00f3n son otro ejemplo de aprendizaje no supervisado. Al proporcionarles datos complejos y sin etiquetas, estos algoritmos generan representaciones 2D o 3D de los datos que se pueden visualizar f\u00e1cilmente. Su objetivo principal es preservar la estructura de los datos tanto como sea posible, evitando que los grupos separados en el espacio de entrada se superpongan en la visualizaci\u00f3n. Esto nos permite comprender c\u00f3mo se organizan los datos y, en ocasiones, identificar patrones que no hab\u00edamos percibido.</p> <p></p> <p>Una tarea relacionada es la reducci\u00f3n de la dimensionalidad, que busca simplificar los datos sin perder informaci\u00f3n esencial. Esto se logra fusionando caracter\u00edsticas correlacionadas en una sola. Por ejemplo, el algoritmo de reducci\u00f3n de la dimensionalidad podr\u00eda combinar el kilometraje y la edad de un autom\u00f3vil en una sola caracter\u00edstica que represente el desgaste del veh\u00edculo. A esto se le llama \"extracci\u00f3n de caracter\u00edsticas\".</p> <p>Otra tarea importante en el aprendizaje no supervisado es la detecci\u00f3n de anomal\u00edas. Esto implica identificar eventos inusuales, como transacciones sospechosas en tarjetas de cr\u00e9dito para prevenir el fraude, detectar defectos en productos de fabricaci\u00f3n o eliminar autom\u00e1ticamente valores at\u00edpicos de un conjunto de datos antes de utilizarlos en otros algoritmos de aprendizaje. Durante el entrenamiento, se presentan principalmente instancias normales al sistema, permiti\u00e9ndole aprender a reconocerlas. Luego, cuando se le presenta una nueva instancia, puede determinar si es una instancia normal o si es probable que sea una anomal\u00eda. Una tarea relacionada es la detecci\u00f3n de novedades, que busca identificar instancias nuevas que son significativamente diferentes de las del conjunto de entrenamiento. Para lograrlo, el conjunto de entrenamiento debe estar \"limpio\", es decir, libre de instancias que se consideren novedades.</p> <p></p> <p>Por \u00faltimo, otra tarea com\u00fan en el aprendizaje no supervisado es el aprendizaje de reglas de asociaci\u00f3n. Aqu\u00ed, el objetivo es explorar grandes vol\u00famenes de datos y descubrir relaciones interesantes entre los atributos. Por ejemplo, si gestionamos un supermercado, aplicar un algoritmo de reglas de asociaci\u00f3n a los registros de ventas podr\u00eda revelar que las personas que compran salsa barbacoa y patatas fritas suelen tambi\u00e9n adquirir bistecs. Esta informaci\u00f3n nos ayudar\u00eda a colocar estos productos cerca uno del otro en la tienda.</p>"},{"location":"0302supervision/#aprendizaje-semisupervisado","title":"Aprendizaje semisupervisado","text":"<p>Dado que etiquetar datos suele ser una tarea laboriosa y costosa, a menudo nos encontramos con un conjunto de datos que contiene muchas instancias sin etiquetar y solo algunas que est\u00e1n etiquetadas. En este escenario, se emplea lo que se conoce como aprendizaje semisupervisado.</p> <p></p> <p>Un ejemplo destacado de esto lo encontramos en algunos servicios de almacenamiento de fotos, como Google Fotos. Cuando subimos nuestras fotos familiares a este servicio, se lleva a cabo autom\u00e1ticamente el reconocimiento de personas. El sistema identifica que una persona (A) aparece en las fotos 1, 5 y 11, mientras que otra persona (B) aparece en las fotos 2, 5 y 7. Esta es la parte no supervisada del algoritmo, que se encarga del agrupamiento. Lo \u00fanico que necesitamos hacer es proporcionar etiquetas para identificar a esas personas. Basta con asignar una etiqueta a cada individuo, y el sistema ser\u00e1 capaz de etiquetar a todos en todas las fotos, lo que resulta muy conveniente para realizar b\u00fasquedas de im\u00e1genes.</p> <p>La mayor\u00eda de los algoritmos de aprendizaje semisupervisado son combinaciones de t\u00e9cnicas no supervisadas y supervisadas. Por ejemplo, se puede emplear un algoritmo de agrupamiento para agrupar instancias similares y, posteriormente, asignar a cada instancia no etiquetada la etiqueta m\u00e1s com\u00fan de su grupo. Una vez que se ha etiquetado todo el conjunto de datos, es posible utilizar cualquier algoritmo de aprendizaje supervisado.</p>"},{"location":"0302supervision/#aprendizaje-por-refuerzo","title":"Aprendizaje por refuerzo","text":"<p>El aprendizaje por refuerzo representa un enfoque completamente distinto. En este contexto, el sistema de aprendizaje, conocido como \"agente\", tiene la capacidad de observar su entorno, tomar decisiones y llevar a cabo acciones, y a cambio recibe \"recompensas\" (o, en algunos casos, castigos en forma de recompensas negativas). El agente debe aprender de manera aut\u00f3noma cu\u00e1l es la estrategia \u00f3ptima, conocida como \"pol\u00edtica\", que le permitir\u00e1 obtener la m\u00e1xima recompensa a lo largo del tiempo. La pol\u00edtica define qu\u00e9 acci\u00f3n debe seleccionar el agente cuando se encuentra en una situaci\u00f3n particular.</p> <p></p> <p>Un ejemplo destacado de esto es la implementaci\u00f3n del aprendizaje por refuerzo en muchos robots que buscan aprender a caminar. El programa AlphaGo de DeepMind tambi\u00e9n representa un excelente ejemplo de aprendizaje por refuerzo. Este programa salt\u00f3 a la fama en mayo de 2017 al derrotar a Ke Jie, quien en ese momento era el n\u00famero uno del mundo en el juego de go. AlphaGo desarroll\u00f3 su pol\u00edtica ganadora al analizar millones de partidas y luego jugar numerosas partidas contra s\u00ed mismo. Es importante se\u00f1alar que durante las partidas contra el campe\u00f3n, el aprendizaje estaba desactivado; AlphaGo simplemente aplicaba la pol\u00edtica que hab\u00eda aprendido. Como veremos m\u00e1s adelante, este enfoque se conoce como aprendizaje offline.</p>"},{"location":"0303lotes/","title":"Aprendizaje por lotes frente a aprendizaje online","text":"<p>Otro criterio utilizado para clasificar sistemas de machine learning es si el sistema puede o no aprender de manera gradual a partir de un flujo de datos entrantes.</p>"},{"location":"0303lotes/#aprendizaje-por-lotes","title":"Aprendizaje por lotes","text":"<p>En el enfoque de aprendizaje por lotes, el sistema no tiene la capacidad de aprender de forma incremental; en su lugar, debe ser entrenado utilizando todos los datos disponibles. Por lo general, este proceso requiere un tiempo significativo y recursos computacionales considerables, por lo que suele realizarse de manera offline. Inicialmente, se entrena el sistema, y una vez completado este proceso, se pone en producci\u00f3n y ejecuta sin continuar aprendiendo; simplemente aplica lo que ha aprendido. A este m\u00e9todo se le conoce como aprendizaje offline.</p> <p>Sin embargo, uno de los desaf\u00edos del aprendizaje por lotes es que con el tiempo, el rendimiento del modelo tiende a decaer gradualmente debido a que el mundo sigue evolucionando mientras el modelo permanece est\u00e1tico. Este fen\u00f3meno se denomina \"data drift\" o deriva de datos. La soluci\u00f3n a este problema implica reentrenar regularmente el modelo con datos actualizados. La frecuencia de reentrenamiento var\u00eda seg\u00fan el caso de uso: por ejemplo, si el modelo clasifica im\u00e1genes de gatos y perros, su rendimiento se deteriorar\u00e1 lentamente, mientras que en aplicaciones financieras, donde los datos cambian r\u00e1pidamente, el deterioro puede ser m\u00e1s r\u00e1pido.</p> <p>Si deseamos que un sistema de aprendizaje por lotes se adapte a nuevos datos, como un nuevo tipo de correo no deseado, debemos entrenar una versi\u00f3n completamente nueva del sistema desde cero, utilizando el conjunto de datos completo (tanto los datos nuevos como los antiguos), y luego reemplazar la versi\u00f3n anterior. Afortunadamente, el proceso completo de entrenamiento, evaluaci\u00f3n y lanzamiento de un sistema de aprendizaje autom\u00e1tico puede automatizarse con relativa facilidad, lo que permite que incluso los sistemas de aprendizaje por lotes se adapten a los cambios. Solo debemos actualizar los datos y reentrenar el sistema con la frecuencia requerida.</p> <p></p> <p>Sin embargo, este enfoque presenta algunos desaf\u00edos. El proceso de entrenamiento en el conjunto de datos completo puede ser intensivo en cuanto a recursos, lo que implica un gasto significativo de recursos computacionales (CPU, memoria, espacio en disco, E/S de disco, E/S de red, etc.). Si los datos son abundantes y automatizamos el proceso de entrenamiento desde cero a diario, el costo puede ser considerable. En casos en los que la cantidad de datos es enorme, incluso puede resultar imposible utilizar un algoritmo de aprendizaje por lotes.</p> <p>Por \u00faltimo, si nuestro sistema necesita aprender de manera continua y opera con recursos limitados, como una aplicaci\u00f3n m\u00f3vil o un rover en Marte, llevar grandes conjuntos de datos de entrenamiento y destinar una gran cantidad de recursos para entrenar durante varias horas al d\u00eda puede ser un problema cr\u00edtico. En estos casos, una opci\u00f3n m\u00e1s adecuada es utilizar algoritmos que sean capaces de aprender de forma incremental.</p>"},{"location":"0303lotes/#aprendizaje-online","title":"Aprendizaje online","text":"<p>En el aprendizaje online, entrenamos el sistema de datos de forma gradual al introducir instancias de datos de manera secuencial, ya sea individualmente o en grupos peque\u00f1os llamados minilotes. Cada paso del aprendizaje es r\u00e1pido y barato, as\u00ed que el sistema puede aprender acerca de datos nuevos sobre la marcha, seg\u00fan llegan.</p> <p></p> <p>El enfoque de aprendizaje online resulta beneficioso para sistemas que requieren una r\u00e1pida adaptaci\u00f3n al cambio, como, por ejemplo, aquellos destinados a la detecci\u00f3n de patrones emergentes en el mercado de valores. Tambi\u00e9n se presenta como una excelente opci\u00f3n en situaciones en las que los recursos computacionales son limitados, como en el caso del entrenamiento de modelos en dispositivos m\u00f3viles.</p> <p>Adem\u00e1s, los algoritmos de aprendizaje online permiten el entrenamiento de sistemas en conjuntos de datos de gran magnitud que no caben en la memoria principal de una m\u00e1quina. Este enfoque se denomina aprendizaje \"out of core\" o \"fuera del n\u00facleo\". El algoritmo carga una porci\u00f3n de los datos, realiza un paso de entrenamiento con esos datos y repite el proceso hasta que ha procesado todos los datos disponibles.</p> <p></p> <p>Un aspecto crucial en los sistemas de aprendizaje online es la velocidad con la que deben adaptarse a los datos cambiantes, lo que se conoce como la \"tasa de aprendizaje\". Si configuramos una tasa de aprendizaje alta, el sistema se ajustar\u00e1 r\u00e1pidamente a los nuevos datos, pero tambi\u00e9n corre el riesgo de olvidar con rapidez los datos anteriores. Este \u00faltimo aspecto no es deseable, especialmente en casos como la detecci\u00f3n de spam, donde no queremos que el sistema se centre solo en los tipos m\u00e1s recientes de spam que ha encontrado. Por otro lado, si optamos por una tasa de aprendizaje baja, el sistema aprender\u00e1 de manera m\u00e1s lenta, pero tambi\u00e9n ser\u00e1 menos susceptible al ruido de los nuevos datos o a secuencias de datos at\u00edpicos.</p> <p>Uno de los desaf\u00edos principales del aprendizaje online radica en la posibilidad de que, si se introducen datos de mala calidad en el sistema, su rendimiento pueda degradarse, posiblemente de manera significativa (esto depende de la calidad de los datos y de la tasa de aprendizaje). Esto es particularmente problem\u00e1tico en sistemas en funcionamiento, ya que los clientes pueden notar r\u00e1pidamente el impacto negativo en su experiencia. Los datos de baja calidad pueden originarse a partir de fallas t\u00e9cnicas, como sensores defectuosos en un robot, o de intentos de manipulaci\u00f3n del sistema, como el bombardeo de spam en un motor de b\u00fasqueda para aumentar su visibilidad en los resultados. Para mitigar este riesgo, es esencial monitorear de cerca el sistema y desactivar el aprendizaje de manera inmediata si se detecta una disminuci\u00f3n en el rendimiento. Tambi\u00e9n es importante establecer mecanismos de monitoreo de los datos de entrada y responder a las se\u00f1ales de datos an\u00f3malos, lo que puede lograrse mediante el uso de algoritmos de detecci\u00f3n de anomal\u00edas.</p>"},{"location":"0304instancias/","title":"Aprendizaje basado en instancias frente a aprendizaje basado en modelos","text":"<p>Otra manera de categorizar los sistemas de machine learning es fijarse en c\u00f3mo generalizan. La mayor\u00eda de las tareas de machine learning tienen que ver con hacer predicciones. Eso significa que, si se le da a un sistema una cantidad de ejemplos de entrenamiento, este tiene que ser capaz de hacer buenas predicciones para (generalizar a) ejemplos que nunca ha visto antes. Tener una buena medida del rendimiento en los datos de entrenamiento est\u00e1 bien, pero no es suficiente; el verdadero objetivo es tener un buen rendimiento en instancias nuevas.</p> <p>Hay dos enfoques principales para la generalizaci\u00f3n: el aprendizaje basado en instancias y el aprendizaje basado en modelos.</p>"},{"location":"0304instancias/#aprendizaje-basado-en-instancias","title":"Aprendizaje basado en instancias","text":"<p>Una forma de aprendizaje bastante sencilla implica la memorizaci\u00f3n pura y simple. Imaginemos que creamos un filtro de spam de esta manera: simplemente marcar\u00eda como spam cualquier correo electr\u00f3nico id\u00e9ntico a los que los usuarios previamente hayan se\u00f1alado como spam. Si bien esta no es la peor soluci\u00f3n, ciertamente no es la m\u00e1s efectiva.</p> <p>En lugar de limitarse a marcar correos id\u00e9nticos a los conocidos como spam, el filtro podr\u00eda programarse para identificar tambi\u00e9n correos que compartan muchas similitudes con correos previamente etiquetados como spam. Para lograrlo, se requiere medir la similitud entre dos correos. Una medida de similitud bastante b\u00e1sica podr\u00eda ser contar la cantidad de palabras en com\u00fan. El sistema etiquetar\u00eda un correo como spam si comparte muchas palabras con un correo de spam previamente registrado.</p> <p>Este enfoque se conoce como \"aprendizaje basado en instancias\". El sistema aprende ejemplos y los almacena en memoria, y luego generaliza para clasificar nuevos casos utilizando una medida de similitud que los compara con los ejemplos aprendidos (o un subconjunto de ellos). Por ejemplo, en la siguiente figura, la nueva instancia se clasificar\u00eda como un tri\u00e1ngulo porque comparte una mayor similitud con ejemplos previos de esa clase.</p> <p></p>"},{"location":"0304instancias/#aprendizaje-basado-en-modelos","title":"Aprendizaje basado en modelos","text":"<p>Otra forma de generalizar a partir de un conjunto de ejemplos es crear un modelo de esos ejemplos y, despu\u00e9s, utilizarlo para hacer predicciones. Esto se denomina \u201caprendizaje basado en modelos\u201d.</p> <p></p>"},{"location":"0401introduccion/","title":"Introducci\u00f3n","text":"<p>La siguiente figura muestra un flujo de trabajo t\u00edpico para un sistema de aprendizaje autom\u00e1tico. Esta figura nos servir\u00e1 de ayuda para detallar las distintas fases necesarias en este tipo de sistema cara a crear finalmente un modelo predictivo.</p> <p></p>"},{"location":"0402preprocesamiento/","title":"Preprocesamiento: c\u00f3mo dar forma a los datos","text":"<p>Los datos en bruto rara vez se presentan en la forma y el formato necesarios para el rendimiento \u00f3ptimo de un algoritmo de aprendizaje. Por ello, el preprocesamiento de los datos es uno de los pasos m\u00e1s importantes en cualquier aplicaci\u00f3n de aprendizaje autom\u00e1tico.</p> <p>El dataset Iris es un ejemplo cl\u00e1sico en el campo de aprendizaje autom\u00e1tico (se puede encontrar m\u00e1s informaci\u00f3n en https://archive.ics.uci.edu/dataset/53/iris. El dataset Iris contiene las medidas de 150 flores de iris de tres especies diferentes: setosa, versicolor y virginica. Aqu\u00ed, cada ejemplo de flor representa una fila en nuestro conjunto de datos, y las medidas de la flor en cent\u00edmetros se almacenan en columnas, a las que tambi\u00e9n llamaremos \u201ccaracter\u00edsticas del dataset\u201d.</p> <p></p> <p>Si tomamos como ejemplo el conjunto de datos de las flores Iris, podemos pensar en los datos brutos como una serie de im\u00e1genes de flores de las queremos extraer caracter\u00edsticas significativas. Las caracter\u00edsticas \u00fatiles podr\u00edan centrarse en el color de las flores, o en su altura, su longitud o su anchura.</p> <p>Muchos algoritmos de aprendizaje autom\u00e1tico tambi\u00e9n requieren, para un rendimiento \u00f3ptimo, que las caracter\u00edsticas seleccionadas est\u00e9n en la misma escala, lo que a menudo se consigue transformando las caracter\u00edsticas en el rango [0, 1], o en una distribuci\u00f3n normal est\u00e1ndar con media cero y varianza unitaria.</p> <p>Algunas de las caracter\u00edsticas seleccionadas pueden estar muy correlacionadas y, por tanto, ser redundantes hasta cierto punto. En estos casos, las t\u00e9cnicas de reducci\u00f3n de la dimensionalidad son \u00fatiles para comprimir las caracter\u00edsticas en un subespacio de menor dimensi\u00f3n. La reducci\u00f3n de la dimensionalidad de este espacio de caracter\u00edsticas tiene la ventaja de que se necesita menos espacio de almacenamiento y el algoritmo de aprendizaje puede funcionar mucho m\u00e1s r\u00e1pido. En ciertos casos, la reducci\u00f3n de la dimensionalidad puede mejorar el rendimiento predictivo de un modelo si el conjunto de datos contiene un gran n\u00famero de caracter\u00edsticas irrelevantes (o ruido); es decir, si el conjunto de datos tiene una baja relaci\u00f3n se\u00f1al-ruido.</p> <p>Para determinar si nuestro algoritmo de aprendizaje autom\u00e1tico no solo funciona bien con el conjunto de datos de entrenamiento, sino que tambi\u00e9n se generaliza bien a nuevos datos, tendremos que dividir aleatoriamente el conjunto de datos en conjunto de datos de entrenamiento y de prueba separados. Utilizamos el conjunto de entrenamiento para entrenar y optimizar nuestro modelo de aprendizaje autom\u00e1tico, y reservamos el conjunto de datos de prueba hasta el final para evaluar el modelo definitivo.</p>"},{"location":"0403entrenamiento/","title":"Entrenamiento y selecci\u00f3n de modelos","text":"<p>Se han desarrollado muchos algoritmos diferentes de aprendizaje autom\u00e1tico para resolver tareas de distintas problem\u00e1ticas. Por ejemplo, cada algoritmo de clasificaci\u00f3n tiene sus sesgos inherentes, y ning\u00fan modelo de clasificaci\u00f3n goza de superioridad si no hacemos ninguna suposici\u00f3n sobre la tarea. En la pr\u00e1ctica, por tanto, es esencial comparar al menos un pu\u00f1ado de algoritmos de aprendizaje diferentes para entrenar, y seleccionar el modelo que mejor funcione. Pero, antes de poder comparar diferentes modelos, tenemos que decidir una m\u00e9trica para medir el rendimiento. Una m\u00e9trica com\u00fanmente utilizada es la precisi\u00f3n de la clasificaci\u00f3n, que se define como la proporci\u00f3n de instancias clasificadas correctamente.</p> <p>\u00bfC\u00f3mo sabemos qu\u00e9 modelo tiene un buen rendimiento con el conjunto de datos de prueba final y con los del mundo real si no utilizamos este conjunto de datos prueba para la selecci\u00f3n del modelo, sino que lo conservamos para la evaluaci\u00f3n final? Para abordar la cuesti\u00f3n que encierra esta pregunta, se pueden utilizar diferentes t\u00e9cnicas, que se resumen en la \u201cvalidaci\u00f3n cruzada\u201d. En la validaci\u00f3n cruzada, dividimos un conjunto de datos en subconjuntos de entrenamiento y validaci\u00f3n para estimar el rendimiento de generalizaci\u00f3n del modelo.</p> <p>Por \u00faltimo, tampoco podemos esperar que los par\u00e1metros por defecto de los diferentes algoritmos de aprendizaje proporcionados por las bibliotecas de software sean \u00f3ptimos para el problema espec\u00edfico de la tarea. Por lo tanto, ser\u00e1 necesario usar con frecuencias t\u00e9cnicas de optimizaci\u00f3n de hiperpar\u00e1metros que nos ayuden a ajustar el rendimiento de nuestro modelo.</p> <p>Podemos pensar en estos hiperpar\u00e1metros como par\u00e1metros que no se deducen de los datos, sino que representan los mandos del modelo sobre los que podemos actuar para mejorar su rendimiento.</p>"},{"location":"0404evaluacion/","title":"Evaluaci\u00f3n de modelos y pron\u00f3stico de instancias de datos ocultos","text":"<p>Despu\u00e9s de seleccionar un modelo que se ha ajustado con el conjunto de datos de entrenamiento, podemos utilizar el conjunto de datos de prueba para estimar su rendimiento con estos datos no vistos, para estimar el llamado error de generalizaci\u00f3n. Si estamos satisfechos con su rendimiento, podemos utilizar este modelo para pronosticar nuevos datos futuros. Es importante tener en cuenta que los par\u00e1metros de los procedimientos mencionados anteriormente, como el escalado de caracter\u00edsticas y la reducci\u00f3n de la dimensionalidad, se obtienen \u00fanicamente a partir del conjunto de datos de entrenamiento, y los mismos par\u00e1metros se vuelven a aplicar posteriormente para transformar el conjunto de datos de prueba, as\u00ed como cualquier instancia de datos nuevos, de lo contrario, el rendimiento medido en los datos de prueba podr\u00eda ser demasiado optimista. </p>"},{"location":"0501introduccion/","title":"Introducci\u00f3n","text":"<p>El aprendizaje autom\u00e1tico supervisado parte de un conjunto de datos de entrenamiento que est\u00e1n previamente etiquetados, en el cual se conoce el valor del atributo objetivo o respuesta. Esto hace que el algoritmo pueda aprender y obtener una funci\u00f3n que sea capaz de generalizar y predecir la respuesta para un conjunto de valores nuevo.</p> <p>Existen dos grandes grupos de m\u00e9todos supervisados: algoritmos de regresi\u00f3n y algoritmos de clasificaci\u00f3n.</p>"},{"location":"0502clasificacion/","title":"Algoritmos de clasificaci\u00f3n","text":"<p>Una tarea de clasificaci\u00f3n se emplea para predecir una categor\u00eda, con un conjunto finito de valores posibles. En el caso de dos posibilidades se trata de clasificaci\u00f3n binaria; y si se quiere predecir m\u00e1s de dos categor\u00edas, se trata de clasificaci\u00f3n multiclase. </p> <p>La frontera de decisi\u00f3n (decision boundary) separa las diferentes clases predichas. Si un algoritmo clasificador aprende una frontera de decisi\u00f3n de tipo lineal, se dice que es un clasificador lineal (lineal classifier).</p> <p>Las siguientes im\u00e1genes muestra un clasificador lineal frente a un no lineal para una tarea de clasificaci\u00f3n binaria (2 clases), que utiliza 2 caracter\u00edsticas. </p> <p></p> <p>Existen clases en la que no es posible utilizar clasificadores lineales con \u00e9xito, ya que no son linealmente separables. La siguiente imagen ejemplifica esto:</p> <p></p> <p>En el caso del dataset Iris tenemos 3 categor\u00edas posibles, por lo que tendr\u00edamos un clasificador multiclase.</p> <p></p> <p>En los anteriores ejemplos se muestran clasificadores que utilizan 2 caracter\u00edsticas, lo que permite representarlos en un plano (ancho por alto). En estos las fronteras de decisi\u00f3n de los clasificadores lineales se visualizan como l\u00edneas rectas.  Esto se puede generalizar para clasificadores que utilicen m\u00e1s 2 caracter\u00edsticas. Con 5 caracter\u00edsticas, el espacio de valores de X es en 5 dimensiones. En este caso, la frontera de decisi\u00f3n ser\u00eda un hiperplano en 4 dimensiones que separar\u00eda el espacio en 2 mitades.</p> <p>La siguiente imagen muestra un clasificador binario con 3 caracter\u00edsticas:</p> <p></p> <p>A continuaci\u00f3n, se detallar\u00e1n algunos algoritmos de clasificaci\u00f3n.</p>"},{"location":"0502clasificacion/#k-vecinos-mas-proximos","title":"K vecinos m\u00e1s pr\u00f3ximos","text":"<p>El algoritmo de k vecinos m\u00e1s pr\u00f3ximos (k-nearest neighbors, k-NN), es un algoritmo supervisado basando en instancias, sin que sea necesario que haya una concordancia exacta con los datos del conjunto de entrenamiento, de forma que el algoritmo clasifica el nuevo elemento en el conjunto que le corresponde buscando en las observaciones m\u00e1s cercanas. Para ello, calcula las distancias a los elementos y las ordena de menor a mayor para asignar el grupo que tenga una frecuencia m\u00e1s alta con las distancias menores.</p> <p>Este m\u00e9todo es muy sensible al par\u00e1metro k, que indica el n\u00famero de vecinos seleccionado y la m\u00e9trica de distancia utilizada. El valor de k se fija tras realizar varias pruebas o mediante una validaci\u00f3n cruzada. La siguiente imagen muestra c\u00f3mo cambia la frontera de decisi\u00f3n conforme se cambia el par\u00e1metro k.</p> <p></p> <p>Como mayor inconveniente, este m\u00e9todo necesita todo el conjunto de datos de entrenamiento para cada elemento nuevo a clasificar, por lo que requiere gran cantidad de memoria y consumo de procesamiento. Por este motivo, es recomendable para conjuntos peque\u00f1os.</p>"},{"location":"0502clasificacion/#arboles-de-decision","title":"\u00c1rboles de decisi\u00f3n","text":"<p>Los \u00e1rboles de decisi\u00f3n (Decision Tree, DT) son algoritmos para hacer clasificaciones realizando particiones sucesivas, mediante la t\u00e9cnica denominada segmentaci\u00f3n jer\u00e1rquica, agrupando observaciones similares. Analizan situaciones que presentan varias posibilidades de decisi\u00f3n y toman la que consideran mejor.</p> <p>El m\u00e9todo, cuando se encuentra ante una decisi\u00f3n, utiliza la informaci\u00f3n del conjunto de datos de entrenamiento para buscar una correlaci\u00f3n entre el elemento nuevo y los conocidos y as\u00ed poder elegir la opci\u00f3n correcta.  Los \u00e1rboles de decisi\u00f3n est\u00e1n formados por los siguientes componentes:</p> <ul> <li>Nodos: son las variables de entrada y plantean las opciones de decisi\u00f3n, siendo el primer elemento del \u00e1rbol el nodo ra\u00edz.</li> <li>Ramas: indican los valores que pueden tomar las variables de entrada y unen los nodos entre s\u00ed.</li> <li>Hojas: muestran los valores de las variables de salida, es decir, el resultado de la decisi\u00f3n.</li> </ul> <p>Algunos de los algoritmos de \u00e1rboles de decisi\u00f3n m\u00e1s utilizados son los siguientes:</p> <ul> <li> <p>El algoritmo de \u00e1rboles de clasificaci\u00f3n y regresi\u00f3n (CART) genera \u00e1rboles de decisi\u00f3n binarios, por lo que cada nodo se divide en dos ramas.   </p> </li> <li> <p>Los algoritmos C5.0, que es una evoluci\u00f3n del C4.5 que a su vez los es de ID3, forman una familia de algoritmos de los m\u00e1s utilizados en tareas de clasificaci\u00f3n.</p> </li> <li>El algoritmo de bosque aleatorio (Random Forest, RF) combina varios \u00e1rboles con distinta cantidad k de caracter\u00edsticas y la salida de cada uno de ellos proporciona un voto, siendo la opci\u00f3n m\u00e1s votada la elegida como la respuesta del modelo.   </li> </ul> <p>Este m\u00e9todo no es apropiado para conjuntos de datos peque\u00f1os y requiere mucho tiempo durante la fase de entrenamiento.</p>"},{"location":"0502clasificacion/#algoritmo-clasificador-bayesiano-ingenuo","title":"Algoritmo clasificador bayesiano ingenuo","text":"<p>El algoritmo clasificador bayesiano ingenuo (Na\u00efve Bayes) es una t\u00e9cnica de clasificaci\u00f3n probabil\u00edstica que est\u00e1 fundamentada en el teorema de Bayes y est\u00e1 basada en la idea de que el mejor modelo es el m\u00e1s probable, asumiendo que la ocurrencia de una determinada caracter\u00edstica es independiente de que sucedan las otras particularidades. Este es el motivo por el que se le llama ingenuo.</p> <p>Solo requiere un peque\u00f1o conjunto de datos de entrenamiento para realizar la estimaci\u00f3n de los par\u00e1metros necesarios: medias y varianzas.</p> <p>Es un algoritmo simple y efectivo que se utiliza frecuentemente en la clasificaci\u00f3n de texto.</p>"},{"location":"0502clasificacion/#analisis-discriminante","title":"An\u00e1lisis discriminante","text":"<p>El an\u00e1lisis discriminante lineal (Lineal Discriminant Analysis, LDA) calcula unas funciones lineales a partir de los atributos de su perfil, donde la funci\u00f3n que alcanza mayor valor define el grupo al que pertenece el nuevo elemento de forma m\u00e1s probable. Cada elemento solamente puede pertenecer a un \u00fanico grupo. </p>"},{"location":"0502clasificacion/#regresion-logistica","title":"Regresi\u00f3n log\u00edstica","text":"<p>Asigna una probabilidad entre 0 y 1 de que una instancia pertenezca a una categor\u00eda. En el caso de una clasificaci\u00f3n binaria una instancia se clasificar\u00eda a una categor\u00eda cuando esta probabilidad sea &gt;= 0.5.</p> <p>Se representa mediante una curva formada con forma de S, llamada funci\u00f3n sigmoidea, que es una funci\u00f3n matem\u00e1tica que se utiliza para asignar los valores predichos a las probabilidades.</p> <p></p>"},{"location":"0502clasificacion/#maquinas-de-vector-soporte","title":"M\u00e1quinas de vector soporte","text":"<p>Las m\u00e1quinas de vector soporte (Support Vector Machines, SVM) separan un grupo dado de datos de entrenamiento etiquetados binarios mediante un hiperplano que est\u00e1 a una distancia m\u00e1xima de ellos (conocido como el hiperplano de margen m\u00e1ximo). De esta forma, elementos que son etiquetados con una categor\u00eda estar\u00e1n a un lado del hiperplano y los casos que se encuentren en la otra categor\u00eda se situar\u00e1n al otro lado. Al vector formado por los puntos m\u00e1s cercanos al hiperplano se le llama vector de soporte.</p> <p></p>"},{"location":"0503regresion/","title":"Algoritmos de regresi\u00f3n","text":"<p>Los modelos de regresi\u00f3n se utilizan para pronosticar variables objetivo en una escala continua, lo que los hace atractivos para abordar muchas cuestiones de la ciencia. Tambi\u00e9n tiene aplicaciones en la industria, como son la comprensi\u00f3n de las relaciones entre variables, la evaluaci\u00f3n de tendencias o la realizaci\u00f3n de previsiones. Un ejemplo es el pron\u00f3stico de las ventas de una empresa en los meses futuros.</p>"},{"location":"0503regresion/#regresion-lineal","title":"Regresi\u00f3n lineal","text":"<p>La regresi\u00f3n lineal simple (univariante) es modelar la relaci\u00f3n entre una \u00fanica caracter\u00edstica (variable explicativa, x) y un objetivo de valor continuo (variable de respuesta, y). La ecuaci\u00f3n de un modelo lineal con una sola variable explicativa se define como sigue: $$ y = w_1 x + b $$</p> <p>, siendo $ b $, la intersecci\u00f3n con el eje $ y $, y $ w_i $ el coeficiente de peso de la variable explicativa.</p> <p>Bas\u00e1ndonos en la ecuaci\u00f3n lineal que hemos definido anteriormente, la regresi\u00f3n lineal puede entenderse como la b\u00fasqueda de la l\u00ednea recta que mejor se ajusta a los ejemplos de entrenamiento.</p> <p></p> <p>Podemos generalizar el modelo de regresi\u00f3n lineal a m\u00faltiples variables explicativas, denomin\u00e1ndose este proceso regresi\u00f3n lineal m\u00faltiple. $$ y = w_1 x_1 + ... + w_m x_m + b $$</p> <p>A continuaci\u00f3n se muestra una imagen de una regresi\u00f3n lineal m\u00faltiple con 2 caracter\u00edstica y 1 valor objetivo.</p> <p></p>"},{"location":"0503regresion/#regresion-polinomica","title":"Regresi\u00f3n polin\u00f3mica","text":"<p>Se puede utilizar un modelo de regresi\u00f3n polin\u00f3mica a\u00f1adiendo t\u00e9rminos polin\u00f3micos: $$ y = w_1 x + w_2 x^2 + \u2026 + w_d x^d + b $$</p> <p>A continuaci\u00f3n se muestra una imagen de una regresi\u00f3n polin\u00f3mica simple con 1 caracter\u00edstica y 1 valor objetivo.</p> <p></p>"},{"location":"0503regresion/#uso-de-algoritmos-de-clasificacion-en-tareas-de-regresion","title":"Uso de algoritmos de clasificaci\u00f3n en tareas de regresi\u00f3n","text":"<p>Los algoritmos vistos en el apartado de clasificaci\u00f3n pueden utilizarse, con ligeras modificaciones, a algoritmos de regresi\u00f3n para la predicci\u00f3n de valores continuos, las m\u00e1quinas de vector de soporte aplicadas a la regresi\u00f3n (SVR), el m\u00e9todo de vecinos m\u00e1s cercanos (k-NN), \u00e1rboles de decisi\u00f3n (DTR) y bosques aleatorios (RFR).</p>"},{"location":"0601introduccion/","title":"Introducci\u00f3n","text":"<p>La validaci\u00f3n de un modelo consta de varios pasos y procesos que garantizan que un modelo funciona como se espera cuando recibe nuevos datos. La forma m\u00e1s com\u00fan de hacer esto es probar la precisi\u00f3n de un modelo en datos que nunca antes hab\u00eda visto (llamado conjunto de prueba). Si la precisi\u00f3n del modelo es similar para los datos con los que se entren\u00f3 y para los datos de prueba, se puede afirmar que el modelo est\u00e1 validado. Sin embargo, la validaci\u00f3n del modelo tambi\u00e9n puede consistir en elegir el modelo correcto, los mejores par\u00e1metros e incluso la mejor m\u00e9trica de precisi\u00f3n. El objetivo final de la validaci\u00f3n de modelos es obtener el modelo con el mejor rendimiento posible, que logre una alta precisi\u00f3n en los datos nuevos. </p>"},{"location":"0601introduccion/#sobreajuste","title":"Sobreajuste","text":"<p>Normalmente, los modelos funcionan mucho mejor con datos que han visto antes, ya que los datos no vistos pueden tener rasgos o caracter\u00edsticas que no fueron expuestas en el modelo. Si los errores de entrenamiento y prueba son muy diferentes, puede ser una se\u00f1al de que el modelo est\u00e1 sobreajustado (overfitting). Usaremos la validaci\u00f3n del modelo para asegurarnos de obtener el mejor error de prueba posible.</p> <p>El sobreajuste ocurre cuando nuestro modelo comienza a darle significado al ruido en los datos de entrenamiento. En el siguiente gr\u00e1fico, se puede ver la forma cuadr\u00e1tica natural de los puntos naranjas. Sin embargo, la l\u00ednea de predicci\u00f3n azul se ajusta demasiado a los datos y probablemente no se extender\u00eda bien a nuevos puntos naranjas. </p> <p></p>"},{"location":"0601introduccion/#subajuste","title":"Subajuste","text":"<p>Si, por el contrario, obtenemos unos errores altos, tanto de entrenamiento como de prueba, puede ser que estemos ante un caso en el que el modelo no ha conseguido encontrar los patrones de relaci\u00f3n entre los datos. En este caso nuestro modelo estar\u00eda subajustado (underfitting).</p> <p>El subajuste ocurre cuando el modelo no pudo encontrar los patrones subyacentes disponibles en los datos. Esto podr\u00eda suceder si no tenemos nuestro modelo subyacente no tiene la suficiente complejidad. La siguiente imagen muestra un ejemplo de modelo subajustado para el conjunto de datos anterior.</p> <p></p>"},{"location":"0602met_regresion/","title":"M\u00e9tricas para modelos de regresi\u00f3n","text":"<p>Los modelos de regresi\u00f3n se construyen para variables continuas. </p>"},{"location":"0602met_regresion/#error-absoluto-medio-mae","title":"Error absoluto medio (MAE)","text":"<p>Para evaluar el desempe\u00f1o de un modelo de regresi\u00f3n, podemos utilizar el error absoluto medio (Mean Absolute Error, MAE). Es la m\u00e9trica de error m\u00e1s simple e intuitiva y es la diferencia absoluta promedio entre las predicciones $ y_i $ y los valores reales $ \\hat{y_i} $. </p> <p>$$ MAE = {\\sum_{i=1}^n \\lvert y_i - \\hat{y}_i \\rvert \\over n} $$</p> <p>Si un perro tuviera seis cachorros, pero hubieras predicho solo cuatro, la diferencia absoluta ser\u00eda dos. Esta m\u00e9trica trata todos los puntos por igual y no es sensible a los valores at\u00edpicos. Cuando se trata de aplicaciones en las que no queremos que los errores grandes tengan un impacto importante, se puede utilizar el error absoluto medio. Un ejemplo podr\u00eda ser predecir la factura mensual de gasolina de un autom\u00f3vil, cuando un valor at\u00edpico puede haber sido causado por un \u00fanico viaje por carretera.</p>"},{"location":"0602met_regresion/#error-cuadratico-medio-mse","title":"Error cuadr\u00e1tico medio (MSE)","text":"<p>El siguiente es el error cuadr\u00e1tico medio (Mean Squared Error, MSE). Es la m\u00e9trica de error de regresi\u00f3n m\u00e1s utilizada para modelos de regresi\u00f3n. Se calcula de manera similar al error absoluto medio, pero esta vez elevamos al cuadrado el t\u00e9rmino de diferencia. </p> <p>$$ MSE = {\\sum_{i=1}^n (y_i - \\hat{y}_i)^2 \\over n} $$</p> <p>El MSE permite que errores mayores tengan un mayor impacto en el modelo. Utilizando el ejemplo anterior del autom\u00f3vil, si se supiera que una vez al a\u00f1o puede realizar un viaje por carretera, se puedea esperar tener de vez en cuando un error grande y se desee que el modelo se recupere de esos viajes.</p> <p>Tambi\u00e9n se suele utilizar la ra\u00edz cuadrada del error cuadr\u00e1tico medio, (Root Mean Squared Error, RMSE).</p> <p>$$ RMSE = \\sqrt{MSE} $$</p>"},{"location":"0602met_regresion/#r-cuadrado","title":"R cuadrado","text":"<p>R cuadrado (R-squared) es una m\u00e9trica que cuantifica la cantidad de varianza en la variable objetivo que se explica por las caracter\u00edsticas. Los valores pueden variar entre 0 y 1, donde uno significa que las caracter\u00edsticas explican completamente la variaci\u00f3n del objetivo. A continuaci\u00f3n hay dos gr\u00e1ficos que visualizan el R cuadrado alto y bajo respectivamente:</p> <p></p>"},{"location":"0603met_clasificacion/","title":"M\u00e9tricas para modelos de clasificaci\u00f3n","text":"<p>Las m\u00e9tricas de precisi\u00f3n de la clasificaci\u00f3n son bastante diferentes a las de regresi\u00f3n. Como se ha comentado anteriormente, los modelos de clasificaci\u00f3n predicen en qu\u00e9 categor\u00eda cae una observaci\u00f3n. Hay muchas m\u00e9tricas de precisi\u00f3n disponibles: precisi\u00f3n, sensibilidad, exactitud, puntuaci\u00f3n F1, etc.</p> <p>Nos centraremos en la precisi\u00f3n, la recuperaci\u00f3n y la exactitud. Ya que cada uno de estos es f\u00e1cil de entender y tiene aplicaciones muy pr\u00e1cticas. Una forma de calcular estas m\u00e9tricas es utilizar los valores de una matriz de confusi\u00f3n.</p>"},{"location":"0603met_clasificacion/#matriz-de-confusion","title":"Matriz de confusi\u00f3n","text":"<p>Al hacer predicciones, especialmente si la clasificaci\u00f3n es binaria, la matriz de confusi\u00f3n (confusion matrix) es uno de los resultados m\u00e1s \u00fatiles a revisar en primera instancia. Cuando tenemos un resultado binario, la matriz de confusi\u00f3n es una matriz de 2x2 que muestra c\u00f3mo se ajustaron las predicciones en los dos resultados. Por ejemplo, para predicciones de 0 que en realidad fueron 0 (o verdaderos negativos), observamos la posici\u00f3n [0, 0] de la matriz. Todas las m\u00e9tricas de precisi\u00f3n mencionados anterioremente se pueden calcular utilizando los valores de esta matriz y es una excelente manera de visualizar los resultados iniciales de un modelo de clasificaci\u00f3n.</p> <p>La siguiente imagen muestra un ejemplo de una matriz de confusi\u00f3n para una clasificaci\u00f3n binaria.</p> <p></p>"},{"location":"0603met_clasificacion/#exactitud","title":"Exactitud","text":"<p>La exactitud (accuracy) es la m\u00e9trica m\u00e1s f\u00e1cil de entender y representa la capacidad general de un modelo para predecir correctamente la clasificaci\u00f3n correcta. Usando la matriz de confusi\u00f3n, se suman los valores que se predijeron como 0 y que en realidad son 0 (verdaderos negativos), a los valores que se predice que son 1 y que son 1 (verdaderos positivos), y luego dividimos por el n\u00famero total de observaciones. En el ejemplo anterior, la exactitud es del 85%.</p> <p></p>"},{"location":"0603met_clasificacion/#precision","title":"Precisi\u00f3n","text":"<p>La precisi\u00f3n (precision) es el n\u00famero de verdaderos positivos entre todos los valores positivos predichos. En el ejemplo anterior se predijeron correctamente 62 valores verdaderos pero tambi\u00e9n se predijo 7 falsos positivos. Por tanto, la precisi\u00f3n es 62 dividido por 69. La precisi\u00f3n se utiliza cuando no queremos sobrepredecir valores positivos. Si el coste de traer en avi\u00f3n a nuevos empleados potenciales es muy elevado, es posible que una empresa solo tenga entrevistas con personas que realmente creen que se unir\u00e1n a su empresa. En los datos del ejemplo, casi 9 de cada 10 predichos se habr\u00edan unido a la empresa.</p> <p></p>"},{"location":"0603met_clasificacion/#sensibilidad","title":"Sensibilidad","text":"<p>La sensibilidad (recall) consiste en encontrar todos los valores positivos. En el anterior ejemplo se predijo correctamente 62 verdaderos positivos y se tuvo 8 falsos negativos. La sensibilidad es de 62 entre 70. La sensibilidad se utiliza cuando no podemos darnos el lujo de perder ning\u00fan valor positivo. Por ejemplo, incluso si un paciente tiene una peque\u00f1a probabilidad de tener c\u00e1ncer, es posible que queramos realizarle pruebas adicionales. El coste de pasar por alto a un paciente que tiene c\u00e1ncer es mucho mayor que el costo de ex\u00e1menes de detecci\u00f3n adicionales para ese paciente.</p> <p></p>"}]}